---
title: "Practical R: Data Ingestion and Munging"
author: Abhijit Dasgupta
date: Fall, 2019

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = '#> ')
library(countdown)
library(fontawesome)
library(tidyverse)
```

layout: true

<div class="my-header">
<span>BIOF339, Fall, 2019</span></div>

---

## Where we left off

We learned how to 

1. `separate` columns into multiple columns based on patterns or index
1. `gather` many columns into 2 columns, with one column having the header information and the other having the values
1. `spread` two columns into many columns, with one column providing the new column headers and the other the values. 

---

## Tidying the weather data

```{r}
library(tidyverse)
weather_data <- rio::import('data/weather.csv')
```

.pull-left[
```{r, echo=F}
head(weather_data)
```
]
.pull-right[

1. Days are in separate columns
1. Temperatures for each day is in two rows, max and min
1. Don't worry about missing values. Just work on getting the shape right
]

---

## Tidying the weather data

.pull-left[
```{r w1, eval = F, echo = T}
weather1 <- tidyr::gather(weather_data, day, temp, -(1:4))
head(weather1,20)
```
]
.pull-right[
```{r, eval=T, echo = F, ref.label="w1"}
```
]

---

## Tidying the weather data

.pull-left[
```{r w2, eval = F, echo = T}
weather1 <- tidyr::gather(weather_data, day, temp, -(1:4))
weather2 <- spread(weather1, element, temp)
head(weather2, 20)
```
]
.pull-right[
```{r, eval=T, echo = F, ref.label="w2"}
```
]

---

## Tidying the weather data

.pull-left[
```{r w3, eval = F, echo = T}
weather1 <- tidyr::gather(weather_data, day, temp, -(1:4))
weather2 <- spread(weather1, element, temp)
weather3 <- separate(weather2, col='day', into=c('symbol','day'), sep=1)
head(weather3)
```

This gets us into the right shape for the data. 

There still is some work to do, but the format is tidy
]
.pull-right[
```{r, eval=T, echo = F, ref.label="w3"}
```
]

---

## Data transformation (dplyr)

The `dplyr` package gives us a few verbs for data manipulation

```{r 01-lecture1-46, echo = F, results='asis'}
dat <- tribble(
  ~Function, ~Purpose,
  'select', "Select columns based on name or position",
  'mutate', 'Create or change a column',
  'filter', 'Extract rows based on some criteria',
  'arrange', 'Re-order rows based on values of variable(s)',
  'group_by', 'Split a dataset by unique values of a variable',
  'summarize', 'Create summary statistics based on columns')
knitr::kable(dat, format='markdown')
```

---

## `select`

You can select columns by name or position, of course. 

You can select consecutive columns using `:` notation, e.g. `select(weather, d1:d31)`

You can also select columns based on some criteria, which are encapsulated in functions.

- `starts_with("___")`, `ends_with("___")`, `contains("____")`
- `one_of("____","_____","______")`

There are others; see `help(starts_with)`.

These selection methods work in all tidyverse functions

> Note that for these functions, the names of the columns don't need to be quoted. This is called *non-standard evaluation* and
is a convenience

---

## select

.pull-left[
```{r w5, eval = F, echo = T}
weather1 <- tidyr::gather(weather_data, day, temp, d1:d31) #<<
head(weather1, 20)
```
]
.pull-right[
```{r, eval=T, echo = F, ref.label="w5"}
```
]

---

## select

.pull-left[
```{r w6, eval = F, echo = T}
weather1 <- tidyr::gather(weather_data, key='day', value='temp', #<<
                   starts_with('d')) #<<
head(weather1, 20)
```
]
.pull-right[
```{r, eval=T, echo = F, ref.label="w6"}
```
]


---

## mutate

.pull-left[
```{r w8, eval = F, echo = T}
weather4 <- mutate(weather3, 
                   num_day = as.numeric(day))
as_tibble(weather4)
```
]
.pull-right[
```{r, eval=T, echo = F, ref.label="w8"}
```
]

---

## mutate

`mutate` can either transform a column in place or create a new column in a dataset

.pull-left[
```{r w7, eval = F, echo = T}
weather4 <- mutate(weather3, day = as.numeric(day))
as_tibble(weather4)
```
]
.pull-right[
```{r, eval=T, echo = F, ref.label="w7"}
```
]

---

## mutate

`mutate` can also be used to deal with missing values, by replacing them with a value, for example

.pull-left[
```{r w11, eval = F, echo = T}
mutate(weather4, tmax = replace_na(tmax, 0))
```

You wouldn't want to do exactly this, of course
]
.pull-right[
```{r, eval=T, echo = F, ref.label="w11"}
```
]
---

## filter

`filter` extracts **rows** based on criteria

So if we wanted to just grab January data, we could use

.pull-left[
```{r w9, eval = F, echo = T}
january <- filter(weather4, month==1)
head(january)
```
]
.pull-right[
```{r, eval=T, echo = F, ref.label="w9"}
```
]

---

## Filter

Some comparison operators for filtering

| Operator | Meaning                          |
|----------|----------------------------------|
| ==       | Equals                           |
| !=       | Not equals                       |
| > / <    | Greater / less than              |
| >= / <=  | Greater or equal / Less or equal |
| !        | Not                              |
| %in%     | In a set                         |

Combining comparisons

| Operator   | Meaning |
|------------|---------|
| &          | And     |
| &#124;       | Or      |

---

## filter

Let's use the `mpg` dataset from the `ggplot2` package

.pull-left[
```{r m1, eval = F, echo = T}
mpg1 <- filter(mpg, 
       (year==1999) & 
         (class %in% c('minivan','suv')))
select(mpg1, manufacturer, cty, hwy, class, year)
```
]
.pull-right[
```{r, eval=T, echo = F, ref.label="m1"}
```
]

---

## filter

A common use of `filter` is to remove rows with missing values from your dataset

.pull-left[
```{r w12, eval = F, echo = T}
weather5 <- filter(weather4, 
                   !is.na(tmax) & !is.na(tmin))
head(weather5, 20)
```

`is.na` is a *function* that tests whether a value is missing or not. 

So `!is.na` is the opposite of that. 
]
.pull-right[
```{r, eval=T, echo = F, ref.label="w12"}
```
]

---

## arrange

`arrange` reorders **rows** of a data set according to the values of one or more variables

.pull-left[
```{r w13, eval = F, echo = T}
arrange(weather5, day)
```

Not quite. 
]
.pull-right[
```{r, eval=T, echo = F, ref.label="w13"}
```
]

---

## arrange

.pull-left[
```{r w14, eval = F, echo = T}
arrange(weather5, month, day)
```
]
.pull-right[
```{r, eval=T, echo = F, ref.label="w14"}
```
]

---

## arrange

1. I use `arrange` sparingly in my workflow
  - For spiffying up final presentation tables
  - If order is **really** important
1. Sorting data is one of the most computationally expensive operations you can do
  - It can crash your computer for big data

---

## Cluttering up our workspace

We've done a bit, but lets see all the objects we've created

```{r}
ls()
```

We see a lot of intermediate datasets we've created, that we aren't going to really use anymore

---
class: middle, center

# Workflow pipes in the tidyverse

---

## Intermediate data sets

Recall how we cleaned the weather dataset yesterday

```{r, eval = F}
weather <- as_tibble(weather)
weather1 <- tidyr::gather(weather, key='day', value = 'temp', starts_with('d'))
weather2 <- spread(weather1, element, temp)
weather3 <- separate(weather2, day, c('symbol','day'), sep = 1)
weather4 <- select(weather3, -symbol)
weather5 <- mutate(weather4, day = as.numeric(day),
                   tmax = replace_na(tmax, 0),
                   tmin = replace_na(tmin, 0))
weather6 <- arrange(weather5,year,month,day)
```

This required us to create and keep track of several intermediate datasets

These datasets are essentially temporary datasets which do not hold the final result

What we did is a series of sequential steps to process the data

---

## The tidyverse pipe

The pipe operator `%>%` was first introduced in the `magrittr` package. 

The idea behind the pipe is to take the result of one function and insert that as an input of another function

In the tidyverse, we start with a tibble, and every intermediate result is also a tibble

---

## The tidyverse pipe

1. The result of each function needs to be a tibble
1. You can omit the input for the data in the code for the function on the right side of the pipe operator, since we know that tits the output from the function to the left of the operator

---

## The tidyverse pipe

```{r, echo=F, results='asis'}
d <- tribble(~"Old School", ~"Piping school",
        "`mutate(mpg, differ = hwy-city))`", "`mpg %>% mutate(differ=hyw-cty)`",
        "`select(mpg, cyl, cty, hwy)`","`mpg %>% select(cyl, cty, hyw)`")

knitr::kable(d, format='markdown')
```

There is a handy shortcut in RStudio to type the pipe operator. It is Ctrl-Shift-M on Windows/Linux and Cmd-Shift-M on Mac.

---

## Reading a pipe

```{r}
mpg %>% 
  mutate(differ = hwy - cty) %>% 
  group_by(cyl) %>% 
  summarize(difference = mean(differ)) %>% 
  ungroup() 
```

You can verbalize the pipe operator as __then__

---

## Reading a pipe

Since the end result of a pipe is a tibble, we can assign a name to store it

```{r}
summary_mpg <- mpg %>% 
  mutate(differ = hwy - cty) %>% 
  group_by(cyl) %>% 
  summarize(difference = mean(differ)) %>% 
  ungroup() 
summary_mpg
```

---

## Exercise

Transform the process of tidyfing the weather data into a pipe format

```{r, echo=F}
countdown(minutes = 10)
```

---

## `group_by` and `summarize`

The `group_by` and `summarize` functions work together, in a principle called 

.center[split-apply-combine]

---

![](img/split-apply-combine.png){

---

## Split-apply-combine

Let's find the average city mileage from the `mpg` dataset by vehicle class

```{r}
mpg %>% 
  group_by(class) %>% 
  summarize(avgMPG = mean(cty))
```


---

## Split-apply-combine

Let's find the average city mileage from the `mpg` dataset by vehicle class

```{r}
mpg %>% 
  group_by(class) %>% 
  summarize(avgMPG = mean(cty)) %>% 
  knitr::kable(format='html') #<<
```

---

![](img/split-apply-combine.png)

---

## Split-apply-combine

We can compute the monthly average minimum and maximum temperatures from the weather dataset

```{r}
weather5 %>% 
  arrange(month, day) %>% 
  group_by(month) %>% 
  summarize(avgMin = mean(tmin, na.rm=T), avgMax = mean(tmax, na.rm=T))
```

---

## Split-apply-combine

The split-apply-combine method is also useful in `mutate`.

In the weather dataset, let's impute the monthly averages for the missing values

```{r}
weather4 %>% # weather4 still has missing values #<<
  arrange(month,day) %>% 
  group_by(month) %>% 
  mutate(tmax = replace_na(tmax, mean(tmax, na.rm=T)),
         tmin = replace_na(tmin, mean(tmin, na.rm=T)))
```

---

## Split-apply-combine

We can even use this principle for some filtering

```{r}
weather5 %>% 
  arrange(month, day) %>% 
  group_by(month) %>% 
  filter(tmax >= mean(tmax, na.rm=T)) %>% #<<
  ungroup()
```

---
class:middle, center, inverse

# Organizing data projects

---

## The data science pipeline

```{r, fig.align='center', out.height=500, echo=F}
knitr::include_graphics('img/0246OS_00_02.png')
```

---

## RStudio Projects

+ RStudio allows you to create [Projects](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)
+ These encapsulate individual projects


---

## Use a template to organize your projects

Before you even get the data

+  Set up a folder structure where
    + you know what goes where
    + you can have canned scripts/packages set things up
+ Make sure it's the same structure every time
+ Next time you visit the project, you don't have to go into desperate search mode

---

<img src="img/template.png" height="600" >

---

## Storing data

1. Keep one copy of the raw data in the format you received it
2. Read in the data into R (see Lecture 7) -> `DataAcquisition.R` 
3. Save a copy of this data in RDSformat (use endings .rds). We'll see how to do this in a few slides

---
## Start working with the data

Summaries:

+ `summary`
+ `dplyr::summarise`
+ `mean`, `sd`, `range`

Exploratory graphs

+ `ggplot`
+ `plot`

Maybe call this file `DataExploration.R`

---

## Data munging 

+ Reshaping data
+ Aggregating data
+ Split-apply-combine

Maybe call this file `DataMunging.R`

---

## Modeling


+ Hypothesis testing
+ Linear models
+ Logistic regression
+ Whatever model you need to run

This process requires a lot of exploration and trial-and-error, so it gets messy
I'll usually create several files that look at different models, but once I'm done, 
my "final" models go in `Modeling.R`

> Coming soon


---

## Packages to be used

You can use several packages in a particular project

It's good practice to load them first, and know what they are

+ Makes sure packages are installed
+ Makes sure package dependencies are met
+ Makes sure package conflicts are known and fixed

---

## Packages to be used

```{r packages, echo=T, eval=F}
reqpackages <- c('dplyr', 'reshape2', 'readxl',
                 'ggplot2','stringr','ggthemes')
for(pkg in reqpackages){
  if(!(pkg %in% row.names(installed.packages()))){
    install.packages(pkg, repos='http://cran.r-project.org')
  }
}

library(dplyr)
library(reshape2)
library(readxl)
library(stringr)
library(ggplot2)
library(ggthemes)
```

This goes in `packages.R`

---

## Creating a pipeline

Now you can ensure that your analyses are reproducible by creating a pipeline where code is run sequentially in a particular order

```{r , echo=T, eval=F}
source('packages.R')
source('DataAcquistion.R')
source('DataExploration.R')
source('DataMunging.R')
source('Modeling.R')
```

Essentially, I'm doing modular programming

+ Separate code by function
+ Makes it easier to debug
+ Try to write code for a particular function once

---

## Saving your work

You would often like to store intermediate datasets, and final datasets, so that you can access them quickly.

There are several ways of saving even large datasets so that they can be quickly accessed. 

| Function  | Package | Example                                  | Re-loading the data                 |
|-----------|---------|------------------------------------------|-------------------------------------|
| saveRDS   | base    | `saveRDS(weather, file = 'weather.rds')` | `weather <- readRDS('weather.rds')` |
| write_fst | fst     | `write_fst(weather, file='weather.fst')` | `weather <- read_fst('weather.fst')` |


